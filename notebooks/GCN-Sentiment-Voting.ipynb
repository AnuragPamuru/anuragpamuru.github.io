{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GinNZkSBeFdC"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/data')\n",
    "from data_loader_sen import data_loader_sen\n",
    "sys.path.append('../src/models')\n",
    "from GCN_model import n_hidden_GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results is when using the sentimental analysis of the tweets and the voting records of 116th congress senators as feature, and the adjacency matrix is the graph of 116th senators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XR6h2vG9L7ls"
   },
   "outputs": [],
   "source": [
    "loader = data_loader_sen( \"../data/voting_features.csv\",\"../data/tweets.csv\", \"../data/edges.csv\")\n",
    "features, labels, A = loader.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Py0zsfIzL-VT",
    "outputId": "a4e4091b-f79f-48b5-8c4b-8c97513c9081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length :70, Validation length :30\n",
      "Epoch: 0\n",
      "training loss 4.9169\n",
      "Validtion: Average loss: 0.0000, Accuracy: 3.3333%\n",
      "Epoch: 1\n",
      "training loss 4.7212\n",
      "Validtion: Average loss: 0.0000, Accuracy: 3.3333%\n",
      "Epoch: 2\n",
      "training loss 4.5374\n",
      "Validtion: Average loss: 0.0000, Accuracy: 3.3333%\n",
      "Epoch: 3\n",
      "training loss 4.3650\n",
      "Validtion: Average loss: 0.0000, Accuracy: 6.6667%\n",
      "Epoch: 4\n",
      "training loss 4.2045\n",
      "Validtion: Average loss: 0.0000, Accuracy: 13.3333%\n",
      "Epoch: 5\n",
      "training loss 4.0542\n",
      "Validtion: Average loss: 0.0000, Accuracy: 20.0000%\n",
      "Epoch: 6\n",
      "training loss 3.9120\n",
      "Validtion: Average loss: 0.0000, Accuracy: 20.0000%\n",
      "Epoch: 7\n",
      "training loss 3.7803\n",
      "Validtion: Average loss: 0.0000, Accuracy: 20.0000%\n",
      "Epoch: 8\n",
      "training loss 3.6584\n",
      "Validtion: Average loss: 0.0000, Accuracy: 20.0000%\n",
      "Epoch: 9\n",
      "training loss 3.5451\n",
      "Validtion: Average loss: 0.0000, Accuracy: 23.3333%\n",
      "Epoch: 10\n",
      "training loss 3.4417\n",
      "Validtion: Average loss: 0.0000, Accuracy: 23.3333%\n",
      "Epoch: 11\n",
      "training loss 3.3481\n",
      "Validtion: Average loss: 0.0000, Accuracy: 23.3333%\n",
      "Epoch: 12\n",
      "training loss 3.2630\n",
      "Validtion: Average loss: 0.0000, Accuracy: 23.3333%\n",
      "Epoch: 13\n",
      "training loss 3.1833\n",
      "Validtion: Average loss: 0.0000, Accuracy: 23.3333%\n",
      "Epoch: 14\n",
      "training loss 3.1069\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 15\n",
      "training loss 3.0327\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 16\n",
      "training loss 2.9600\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 17\n",
      "training loss 2.8882\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 18\n",
      "training loss 2.8175\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 19\n",
      "training loss 2.7478\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 20\n",
      "training loss 2.6789\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 21\n",
      "training loss 2.6108\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 22\n",
      "training loss 2.5438\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 23\n",
      "training loss 2.4781\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 24\n",
      "training loss 2.4141\n",
      "Validtion: Average loss: 0.0000, Accuracy: 26.6667%\n",
      "Epoch: 25\n",
      "training loss 2.3522\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 26\n",
      "training loss 2.2928\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 27\n",
      "training loss 2.2361\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 28\n",
      "training loss 2.1820\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 29\n",
      "training loss 2.1307\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 30\n",
      "training loss 2.0828\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 31\n",
      "training loss 2.0377\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 32\n",
      "training loss 1.9956\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 33\n",
      "training loss 1.9566\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 34\n",
      "training loss 1.9200\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 35\n",
      "training loss 1.8858\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 36\n",
      "training loss 1.8533\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 37\n",
      "training loss 1.8223\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 38\n",
      "training loss 1.7929\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 39\n",
      "training loss 1.7649\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 40\n",
      "training loss 1.7380\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 41\n",
      "training loss 1.7117\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 42\n",
      "training loss 1.6861\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 43\n",
      "training loss 1.6613\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 44\n",
      "training loss 1.6371\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 45\n",
      "training loss 1.6137\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 46\n",
      "training loss 1.5909\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 47\n",
      "training loss 1.5687\n",
      "Validtion: Average loss: 0.0000, Accuracy: 30.0000%\n",
      "Epoch: 48\n",
      "training loss 1.5470\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 49\n",
      "training loss 1.5261\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 50\n",
      "training loss 1.5057\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 51\n",
      "training loss 1.4859\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 52\n",
      "training loss 1.4669\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 53\n",
      "training loss 1.4475\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 54\n",
      "training loss 1.4284\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 55\n",
      "training loss 1.4092\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 56\n",
      "training loss 1.3895\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 57\n",
      "training loss 1.3700\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 58\n",
      "training loss 1.3502\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 59\n",
      "training loss 1.3296\n",
      "Validtion: Average loss: 0.0000, Accuracy: 33.3333%\n",
      "Epoch: 60\n",
      "training loss 1.3076\n",
      "Validtion: Average loss: 0.0000, Accuracy: 36.6667%\n",
      "Epoch: 61\n",
      "training loss 1.2856\n",
      "Validtion: Average loss: 0.0000, Accuracy: 36.6667%\n",
      "Epoch: 62\n",
      "training loss 1.2643\n",
      "Validtion: Average loss: 0.0000, Accuracy: 40.0000%\n",
      "Epoch: 63\n",
      "training loss 1.2434\n",
      "Validtion: Average loss: 0.0000, Accuracy: 40.0000%\n",
      "Epoch: 64\n",
      "training loss 1.2227\n",
      "Validtion: Average loss: 0.0000, Accuracy: 40.0000%\n",
      "Epoch: 65\n",
      "training loss 1.2021\n",
      "Validtion: Average loss: 0.0000, Accuracy: 43.3333%\n",
      "Epoch: 66\n",
      "training loss 1.1823\n",
      "Validtion: Average loss: 0.0000, Accuracy: 46.6667%\n",
      "Epoch: 67\n",
      "training loss 1.1621\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 68\n",
      "training loss 1.1405\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 69\n",
      "training loss 1.1187\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 70\n",
      "training loss 1.0954\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 71\n",
      "training loss 1.0732\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 72\n",
      "training loss 1.0445\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.0000%\n",
      "Epoch: 73\n",
      "training loss 1.0057\n",
      "Validtion: Average loss: 0.0000, Accuracy: 53.3333%\n",
      "Epoch: 74\n",
      "training loss 0.9630\n",
      "Validtion: Average loss: 0.0000, Accuracy: 53.3333%\n",
      "Epoch: 75\n",
      "training loss 0.9209\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.0000%\n",
      "Epoch: 76\n",
      "training loss 0.8833\n",
      "Validtion: Average loss: 0.0000, Accuracy: 53.3333%\n",
      "Epoch: 77\n",
      "training loss 0.8505\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.0000%\n",
      "Epoch: 78\n",
      "training loss 0.8185\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.0000%\n",
      "Epoch: 79\n",
      "training loss 0.7886\n",
      "Validtion: Average loss: 0.0000, Accuracy: 56.6667%\n",
      "Epoch: 80\n",
      "training loss 0.7602\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.0000%\n",
      "Epoch: 81\n",
      "training loss 0.7357\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.0000%\n",
      "Epoch: 82\n",
      "training loss 0.7144\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 83\n",
      "training loss 0.6953\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 84\n",
      "training loss 0.6770\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 85\n",
      "training loss 0.6596\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 86\n",
      "training loss 0.6428\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 87\n",
      "training loss 0.6241\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 88\n",
      "training loss 0.6017\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 89\n",
      "training loss 0.5784\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 90\n",
      "training loss 0.5551\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 91\n",
      "training loss 0.5328\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 92\n",
      "training loss 0.5126\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 93\n",
      "training loss 0.4951\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.3333%\n",
      "Epoch: 94\n",
      "training loss 0.4808\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 95\n",
      "training loss 0.4692\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 96\n",
      "training loss 0.4593\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 97\n",
      "training loss 0.4505\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 98\n",
      "training loss 0.4423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 99\n",
      "training loss 0.4344\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 100\n",
      "training loss 0.4267\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 101\n",
      "training loss 0.4191\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 102\n",
      "training loss 0.4116\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 103\n",
      "training loss 0.4043\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 104\n",
      "training loss 0.3972\n",
      "Validtion: Average loss: 0.0000, Accuracy: 66.6667%\n",
      "Epoch: 105\n",
      "training loss 0.3904\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 106\n",
      "training loss 0.3838\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 107\n",
      "training loss 0.3775\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 108\n",
      "training loss 0.3715\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 109\n",
      "training loss 0.3659\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 110\n",
      "training loss 0.3606\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.0000%\n",
      "Epoch: 111\n",
      "training loss 0.3555\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 112\n",
      "training loss 0.3508\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 113\n",
      "training loss 0.3464\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 114\n",
      "training loss 0.3421\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 115\n",
      "training loss 0.3381\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 116\n",
      "training loss 0.3343\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 117\n",
      "training loss 0.3306\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 118\n",
      "training loss 0.3270\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 119\n",
      "training loss 0.3235\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 120\n",
      "training loss 0.3199\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 121\n",
      "training loss 0.3161\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 122\n",
      "training loss 0.3123\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 123\n",
      "training loss 0.3085\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 124\n",
      "training loss 0.3047\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 125\n",
      "training loss 0.3009\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 126\n",
      "training loss 0.2972\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 127\n",
      "training loss 0.2936\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 128\n",
      "training loss 0.2901\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 129\n",
      "training loss 0.2866\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 130\n",
      "training loss 0.2832\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 131\n",
      "training loss 0.2800\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 132\n",
      "training loss 0.2767\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 133\n",
      "training loss 0.2736\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 134\n",
      "training loss 0.2706\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 135\n",
      "training loss 0.2676\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 136\n",
      "training loss 0.2647\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 137\n",
      "training loss 0.2618\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 138\n",
      "training loss 0.2591\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 139\n",
      "training loss 0.2564\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 140\n",
      "training loss 0.2537\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 141\n",
      "training loss 0.2511\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 142\n",
      "training loss 0.2485\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 143\n",
      "training loss 0.2460\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 144\n",
      "training loss 0.2436\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 145\n",
      "training loss 0.2412\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 146\n",
      "training loss 0.2388\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 147\n",
      "training loss 0.2365\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 148\n",
      "training loss 0.2343\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 149\n",
      "training loss 0.2321\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 150\n",
      "training loss 0.2299\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 151\n",
      "training loss 0.2278\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 152\n",
      "training loss 0.2258\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 153\n",
      "training loss 0.2237\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 154\n",
      "training loss 0.2217\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 155\n",
      "training loss 0.2198\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 156\n",
      "training loss 0.2179\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 157\n",
      "training loss 0.2156\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 158\n",
      "training loss 0.2127\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 159\n",
      "training loss 0.2095\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 160\n",
      "training loss 0.2061\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 161\n",
      "training loss 0.2025\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 162\n",
      "training loss 0.1989\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 163\n",
      "training loss 0.1952\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 164\n",
      "training loss 0.1916\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 165\n",
      "training loss 0.1881\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 166\n",
      "training loss 0.1848\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 167\n",
      "training loss 0.1816\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 168\n",
      "training loss 0.1786\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 169\n",
      "training loss 0.1759\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 170\n",
      "training loss 0.1733\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 171\n",
      "training loss 0.1708\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 172\n",
      "training loss 0.1685\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 173\n",
      "training loss 0.1663\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 174\n",
      "training loss 0.1643\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3333%\n",
      "Epoch: 175\n",
      "training loss 0.1624\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 176\n",
      "training loss 0.1605\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 177\n",
      "training loss 0.1587\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 178\n",
      "training loss 0.1571\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 179\n",
      "training loss 0.1554\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 180\n",
      "training loss 0.1539\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 181\n",
      "training loss 0.1524\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.6667%\n",
      "Epoch: 182\n",
      "training loss 0.1509\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 183\n",
      "training loss 0.1495\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 184\n",
      "training loss 0.1482\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 185\n",
      "training loss 0.1468\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 186\n",
      "training loss 0.1456\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 187\n",
      "training loss 0.1444\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 188\n",
      "training loss 0.1432\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 189\n",
      "training loss 0.1421\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 190\n",
      "training loss 0.1410\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 191\n",
      "training loss 0.1399\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 192\n",
      "training loss 0.1389\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 193\n",
      "training loss 0.1379\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 194\n",
      "training loss 0.1369\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 195\n",
      "training loss 0.1359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 196\n",
      "training loss 0.1350\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 197\n",
      "training loss 0.1341\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 198\n",
      "training loss 0.1332\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 199\n",
      "training loss 0.1323\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 200\n",
      "training loss 0.1315\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 201\n",
      "training loss 0.1306\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 202\n",
      "training loss 0.1298\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 203\n",
      "training loss 0.1290\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 204\n",
      "training loss 0.1282\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 205\n",
      "training loss 0.1274\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 206\n",
      "training loss 0.1267\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 207\n",
      "training loss 0.1259\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 208\n",
      "training loss 0.1252\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 209\n",
      "training loss 0.1245\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 210\n",
      "training loss 0.1238\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 211\n",
      "training loss 0.1231\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 212\n",
      "training loss 0.1224\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 213\n",
      "training loss 0.1217\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 214\n",
      "training loss 0.1210\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 215\n",
      "training loss 0.1204\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 216\n",
      "training loss 0.1197\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 217\n",
      "training loss 0.1191\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 218\n",
      "training loss 0.1185\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 219\n",
      "training loss 0.1179\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 220\n",
      "training loss 0.1172\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 221\n",
      "training loss 0.1166\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 222\n",
      "training loss 0.1160\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 223\n",
      "training loss 0.1154\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 224\n",
      "training loss 0.1149\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 225\n",
      "training loss 0.1143\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 226\n",
      "training loss 0.1137\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 227\n",
      "training loss 0.1131\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 228\n",
      "training loss 0.1126\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 229\n",
      "training loss 0.1120\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 230\n",
      "training loss 0.1115\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.0000%\n",
      "Epoch: 231\n",
      "training loss 0.1109\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 232\n",
      "training loss 0.1104\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 233\n",
      "training loss 0.1099\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 234\n",
      "training loss 0.1094\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 235\n",
      "training loss 0.1088\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 236\n",
      "training loss 0.1083\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 237\n",
      "training loss 0.1078\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 238\n",
      "training loss 0.1073\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 239\n",
      "training loss 0.1068\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 240\n",
      "training loss 0.1063\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 241\n",
      "training loss 0.1058\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 242\n",
      "training loss 0.1054\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 243\n",
      "training loss 0.1049\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 244\n",
      "training loss 0.1044\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 245\n",
      "training loss 0.1040\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 246\n",
      "training loss 0.1035\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 247\n",
      "training loss 0.1031\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 248\n",
      "training loss 0.1026\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 249\n",
      "training loss 0.1022\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 250\n",
      "training loss 0.1017\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 251\n",
      "training loss 0.1013\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 252\n",
      "training loss 0.1009\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 253\n",
      "training loss 0.1004\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 254\n",
      "training loss 0.1000\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 255\n",
      "training loss 0.0996\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 256\n",
      "training loss 0.0992\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 257\n",
      "training loss 0.0988\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 258\n",
      "training loss 0.0984\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 259\n",
      "training loss 0.0980\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 260\n",
      "training loss 0.0976\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 261\n",
      "training loss 0.0972\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 262\n",
      "training loss 0.0968\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 263\n",
      "training loss 0.0964\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 264\n",
      "training loss 0.0961\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 265\n",
      "training loss 0.0957\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 266\n",
      "training loss 0.0953\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 267\n",
      "training loss 0.0949\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 268\n",
      "training loss 0.0946\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 269\n",
      "training loss 0.0942\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 270\n",
      "training loss 0.0939\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 271\n",
      "training loss 0.0935\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 272\n",
      "training loss 0.0932\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 273\n",
      "training loss 0.0928\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 274\n",
      "training loss 0.0925\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 275\n",
      "training loss 0.0922\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 276\n",
      "training loss 0.0918\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 277\n",
      "training loss 0.0915\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 278\n",
      "training loss 0.0912\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 279\n",
      "training loss 0.0908\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 280\n",
      "training loss 0.0905\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 281\n",
      "training loss 0.0902\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 282\n",
      "training loss 0.0899\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 283\n",
      "training loss 0.0896\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 284\n",
      "training loss 0.0893\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 285\n",
      "training loss 0.0890\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 286\n",
      "training loss 0.0887\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 287\n",
      "training loss 0.0884\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 288\n",
      "training loss 0.0881\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 289\n",
      "training loss 0.0878\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 290\n",
      "training loss 0.0875\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 291\n",
      "training loss 0.0872\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 292\n",
      "training loss 0.0869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 293\n",
      "training loss 0.0866\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 294\n",
      "training loss 0.0863\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 295\n",
      "training loss 0.0861\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 296\n",
      "training loss 0.0858\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 297\n",
      "training loss 0.0855\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 298\n",
      "training loss 0.0852\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 299\n",
      "training loss 0.0850\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 300\n",
      "training loss 0.0847\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 301\n",
      "training loss 0.0844\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 302\n",
      "training loss 0.0842\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 303\n",
      "training loss 0.0839\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 304\n",
      "training loss 0.0837\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 305\n",
      "training loss 0.0834\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 306\n",
      "training loss 0.0831\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 307\n",
      "training loss 0.0829\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 308\n",
      "training loss 0.0826\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 309\n",
      "training loss 0.0824\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 310\n",
      "training loss 0.0822\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 311\n",
      "training loss 0.0819\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 312\n",
      "training loss 0.0817\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 313\n",
      "training loss 0.0814\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 314\n",
      "training loss 0.0812\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 315\n",
      "training loss 0.0809\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 316\n",
      "training loss 0.0807\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 317\n",
      "training loss 0.0805\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 318\n",
      "training loss 0.0802\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 319\n",
      "training loss 0.0800\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 320\n",
      "training loss 0.0798\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 321\n",
      "training loss 0.0795\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 322\n",
      "training loss 0.0793\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 323\n",
      "training loss 0.0791\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 324\n",
      "training loss 0.0789\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 325\n",
      "training loss 0.0786\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 326\n",
      "training loss 0.0784\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 327\n",
      "training loss 0.0782\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 328\n",
      "training loss 0.0780\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 329\n",
      "training loss 0.0778\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 330\n",
      "training loss 0.0775\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 331\n",
      "training loss 0.0773\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 332\n",
      "training loss 0.0771\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 333\n",
      "training loss 0.0769\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 334\n",
      "training loss 0.0767\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 335\n",
      "training loss 0.0765\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 336\n",
      "training loss 0.0763\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 337\n",
      "training loss 0.0761\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 338\n",
      "training loss 0.0759\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 339\n",
      "training loss 0.0757\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 340\n",
      "training loss 0.0755\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 341\n",
      "training loss 0.0753\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 342\n",
      "training loss 0.0751\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 343\n",
      "training loss 0.0749\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 344\n",
      "training loss 0.0747\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 345\n",
      "training loss 0.0745\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 346\n",
      "training loss 0.0743\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 347\n",
      "training loss 0.0741\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 348\n",
      "training loss 0.0739\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 349\n",
      "training loss 0.0737\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 350\n",
      "training loss 0.0735\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 351\n",
      "training loss 0.0734\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 352\n",
      "training loss 0.0732\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 353\n",
      "training loss 0.0730\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 354\n",
      "training loss 0.0728\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 355\n",
      "training loss 0.0726\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 356\n",
      "training loss 0.0725\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 357\n",
      "training loss 0.0723\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 358\n",
      "training loss 0.0721\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 359\n",
      "training loss 0.0719\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 360\n",
      "training loss 0.0718\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 361\n",
      "training loss 0.0716\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 362\n",
      "training loss 0.0714\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 363\n",
      "training loss 0.0712\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 364\n",
      "training loss 0.0711\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 365\n",
      "training loss 0.0709\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 366\n",
      "training loss 0.0707\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 367\n",
      "training loss 0.0706\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 368\n",
      "training loss 0.0704\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 369\n",
      "training loss 0.0702\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 370\n",
      "training loss 0.0701\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 371\n",
      "training loss 0.0699\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 372\n",
      "training loss 0.0697\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 373\n",
      "training loss 0.0696\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 374\n",
      "training loss 0.0694\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 375\n",
      "training loss 0.0693\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 376\n",
      "training loss 0.0691\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 377\n",
      "training loss 0.0690\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 378\n",
      "training loss 0.0688\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 379\n",
      "training loss 0.0686\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 380\n",
      "training loss 0.0685\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 381\n",
      "training loss 0.0683\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 382\n",
      "training loss 0.0682\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 383\n",
      "training loss 0.0680\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 384\n",
      "training loss 0.0679\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 385\n",
      "training loss 0.0677\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 386\n",
      "training loss 0.0675\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 387\n",
      "training loss 0.0674\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 388\n",
      "training loss 0.0672\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 389\n",
      "training loss 0.0671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 390\n",
      "training loss 0.0669\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 391\n",
      "training loss 0.0668\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 392\n",
      "training loss 0.0666\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 393\n",
      "training loss 0.0664\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 394\n",
      "training loss 0.0663\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 395\n",
      "training loss 0.0661\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 396\n",
      "training loss 0.0659\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3333%\n",
      "Epoch: 397\n",
      "training loss 0.0658\n",
      "Validtion: Average loss: 0.0000, Accuracy: 86.6667%\n",
      "Epoch: 398\n",
      "training loss 0.0656\n",
      "Validtion: Average loss: 0.0000, Accuracy: 86.6667%\n",
      "Epoch: 399\n",
      "training loss 0.0654\n",
      "Validtion: Average loss: 0.0000, Accuracy: 86.6667%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [3.3333333333333335,\n",
       "  3.3333333333333335,\n",
       "  3.3333333333333335,\n",
       "  6.666666666666667,\n",
       "  13.333333333333334,\n",
       "  20.0,\n",
       "  20.0,\n",
       "  20.0,\n",
       "  20.0,\n",
       "  23.333333333333332,\n",
       "  23.333333333333332,\n",
       "  23.333333333333332,\n",
       "  23.333333333333332,\n",
       "  23.333333333333332,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  26.666666666666668,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  30.0,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  33.333333333333336,\n",
       "  36.666666666666664,\n",
       "  36.666666666666664,\n",
       "  40.0,\n",
       "  40.0,\n",
       "  40.0,\n",
       "  43.333333333333336,\n",
       "  46.666666666666664,\n",
       "  50.0,\n",
       "  50.0,\n",
       "  50.0,\n",
       "  50.0,\n",
       "  50.0,\n",
       "  50.0,\n",
       "  53.333333333333336,\n",
       "  53.333333333333336,\n",
       "  60.0,\n",
       "  53.333333333333336,\n",
       "  60.0,\n",
       "  60.0,\n",
       "  56.666666666666664,\n",
       "  60.0,\n",
       "  60.0,\n",
       "  63.333333333333336,\n",
       "  63.333333333333336,\n",
       "  63.333333333333336,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  63.333333333333336,\n",
       "  63.333333333333336,\n",
       "  63.333333333333336,\n",
       "  63.333333333333336,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  70.0,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  73.33333333333333,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  76.66666666666667,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  80.0,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  83.33333333333333,\n",
       "  86.66666666666667,\n",
       "  86.66666666666667,\n",
       "  86.66666666666667]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = n_hidden_GCN(A, features, labels, hidden_neurons=200, F = 1079, val_size=0.3)\n",
    "model.train_epoch(epochs=400, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V9cpRMDmLfAh"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 100 elements, which is not acceptable for use with 'x' with size 99, 'y' with size 99.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xshape, yshape, get_next_color_func)\u001b[0m\n\u001b[0;32m   4284\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Then is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4285\u001b[1;33m                 \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4286\u001b[0m                 \u001b[0mn_elem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: RGBA sequence should have length 3 or 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-012df6652427>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\WI21\\b01-group3-pa\\src\\models\\GCN_model.py\u001b[0m in \u001b[0;36mvisualization\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m675\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mdraw_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx\u001b[1;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# default to spring layout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m     \u001b[0mnode_collection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_networkx_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m     \u001b[0medge_collection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_networkx_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx_nodes\u001b[1;34m(G, pos, nodelist, node_size, node_color, node_shape, alpha, cmap, vmin, vmax, ax, linewidths, edgecolors, label, **kwds)\u001b[0m\n\u001b[0;32m    412\u001b[0m                                  \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                                  \u001b[0medgecolors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                                  label=label)\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[0mnode_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_zorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4452\u001b[0m             self._parse_scatter_color_args(\n\u001b[0;32m   4453\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4454\u001b[1;33m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n\u001b[0m\u001b[0;32m   4455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4456\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplotnonfinite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xshape, yshape, get_next_color_func)\u001b[0m\n\u001b[0;32m   4296\u001b[0m                         \u001b[1;34m\"acceptable for use with 'x' with size {xs}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4297\u001b[0m                         \u001b[1;34m\"'y' with size {ys}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4298\u001b[1;33m                             \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_elem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mysize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4299\u001b[0m                     )\n\u001b[0;32m   4300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'c' argument has 100 elements, which is not acceptable for use with 'x' with size 99, 'y' with size 99."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GCN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
